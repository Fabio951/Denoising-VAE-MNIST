{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_some(dataset):\n",
    "    ### Plot some sample\n",
    "    plt.close('all')\n",
    "    fig, axs = plt.subplots(5, 5, figsize=(8,8))\n",
    "    for ax in axs.flatten():\n",
    "        img, label = random.choice(dataset)\n",
    "        ax.imshow(img.squeeze().numpy(), cmap='gist_gray')\n",
    "        ax.set_title('Label: %d' % label)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(data_dir):\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    train_dataset = MNIST(data_dir, train=True,  download=True, transform=train_transform)\n",
    "    test_dataset  = MNIST(data_dir, train=False, download=True, transform=test_transform)\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net, dataloader, loss_fn, optimizer, show_steps=1, use_noise=False):\n",
    "    net.train()\n",
    "    counter = 0\n",
    "    for sample_batch in dataloader:\n",
    "        if use_noise:\n",
    "            transform_noise = transforms.Lambda(lambda x: x + 0.5*torch.cuda.FloatTensor(x.shape).normal_())\n",
    "            transform_crop = transforms.RandomErasing(p=1, scale=(0.01,0.25), ratio=(0.5,2))\n",
    "            transform = transforms.Compose([transform_crop,\n",
    "                                            transform_noise])\n",
    "            image_batch = sample_batch[0].to('cuda')\n",
    "            image_batch_or = image_batch.clone()\n",
    "            for i in range(len(image_batch)):\n",
    "                image_batch[i] = transform(image_batch[i])\n",
    "        else:\n",
    "            image_batch = sample_batch[0].to('cuda')\n",
    "            image_batch_or = image_batch.clone()\n",
    "        output = net(image_batch)\n",
    "        loss = loss_fn(output, image_batch_or)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        counter += 1\n",
    "        if counter%show_steps==0:\n",
    "            print('\\t partial train loss: %f' % (loss.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(net, dataloader, loss_fn):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        conc_out = torch.Tensor().float()\n",
    "        conc_label = torch.Tensor().float()\n",
    "        for sample_batch in dataloader:\n",
    "            image_batch = sample_batch[0].to('cuda')\n",
    "            out = net(image_batch)\n",
    "            conc_out = torch.cat([conc_out, out.cpu()])\n",
    "            conc_label = torch.cat([conc_label, image_batch.cpu()])\n",
    "        val_loss = loss_fn(conc_out, conc_label)\n",
    "    return val_loss.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the progress of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_progress(index, dataset, net, epoch):\n",
    "    img = dataset[index][0].unsqueeze(0).to('cuda')\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        rec_img  = net(img)\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8,4))\n",
    "    axs[0].imshow(img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "    axs[0].set_title('Original image')\n",
    "    axs[1].imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "    axs[1].set_title('Reconstructed image (EPOCH %d)' % (epoch + 1))\n",
    "    plt.tight_layout()\n",
    "    plt.pause(0.1)\n",
    "    # Save figures\n",
    "    os.makedirs('progress/autoencoder_progress_%d_features' % net.encoded_space_dim, exist_ok=True)\n",
    "    plt.savefig('progress/autoencoder_progress_%d_features/epoch_%d.png' % (net.encoded_space_dim, epoch + 1))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show structure of latent space (so far just 2-dim latent space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(net, dataset):\n",
    "    encoded_samples = []\n",
    "    for sample in dataset:\n",
    "        img = sample[0].unsqueeze(0)\n",
    "        label = sample[1]\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            encoded_img  = net.encode(img)\n",
    "        encoded_samples.append((encoded_img.flatten().numpy(), label))\n",
    "        \n",
    "    color_map = {\n",
    "        0: '#1f77b4',\n",
    "        1: '#ff7f0e',\n",
    "        2: '#2ca02c',\n",
    "        3: '#d62728',\n",
    "        4: '#9467bd',\n",
    "        5: '#8c564b',\n",
    "        6: '#e377c2',\n",
    "        7: '#7f7f7f',\n",
    "        8: '#bcbd22',\n",
    "        9: '#17becf'\n",
    "        }\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12,10))\n",
    "    for enc_sample, label in encoded_samples:\n",
    "        ax.plot(enc_sample[0], enc_sample[1], marker='.', color=color_map[label])\n",
    "    plt.xlim(-8,8)\n",
    "    plt.ylim(-8,8)\n",
    "    plt.close()\n",
    "    \n",
    "    #x_max = max(encoded_samples, key = lambda k: k[0][0])[0][0]\n",
    "    #x_min = min(encoded_samples, key = lambda k: k[0][0])[0][0]\n",
    "    #y_max = max(encoded_samples, key = lambda k: k[0][1])[0][1]\n",
    "    #y_min = min(encoded_samples, key = lambda k: k[0][1])[0][1]\n",
    "    \n",
    "    digit_size = 28\n",
    "    n_digits = 25\n",
    "    figure = np.zeros((digit_size*n_digits, digit_size*n_digits))\n",
    "    grid_x = np.linspace(ax.get_xlim()[0], ax.get_xlim()[1], n_digits)\n",
    "    grid_y = np.linspace(ax.get_ylim()[1], ax.get_ylim()[0], n_digits)\n",
    "    \n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = torch.Tensor([xi, yi])\n",
    "            x_decoded = net.decode(z_sample)\n",
    "            digit = x_decoded.view(digit_size, digit_size)\n",
    "            figure[i*digit_size: (i+1)*digit_size, \n",
    "                   j*digit_size: (j+1)*digit_size] = digit.detach().numpy()\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    plt.imshow(figure, cmap='Greys', alpha=0.7, extent=[ax.get_xlim()[0], ax.get_xlim()[1],\n",
    "                                                        ax.get_ylim()[0], ax.get_ylim()[1]])\n",
    "    for enc_sample, label in encoded_samples:\n",
    "        plt.plot(enc_sample[0], enc_sample[1], marker='.', color=color_map[label], alpha=0.2)\n",
    "    plt.grid(True)\n",
    "    plt.legend([plt.Line2D([0], [0], ls='', marker='.', color=c, label=l) for l, c in color_map.items()],\n",
    "               color_map.keys())\n",
    "    plt.tight_layout()\n",
    "    plt.xlim(-8,8)\n",
    "    plt.ylim(-8,8)\n",
    "    plt.title('Latent Space')\n",
    "    fig.savefig('report/latent_2.pdf')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for computing the loss over different levels of noise and crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_limits_loss_noise(net, dataloader, loss_fn):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        conc_out = torch.Tensor().float()\n",
    "        conc_label = torch.Tensor().float()\n",
    "        for sample_batch in dataloader:\n",
    "            image_batch = sample_batch[0]\n",
    "            out = net(image_batch)\n",
    "            conc_out = torch.cat([conc_out, out.cpu()])\n",
    "            conc_label = torch.cat([conc_label, image_batch.cpu()])\n",
    "        val_loss = loss_fn(conc_out, conc_label)\n",
    "    min_val = val_loss.item()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        conc_out = torch.Tensor().float()\n",
    "        conc_label = torch.Tensor().float()\n",
    "        for sample_batch in dataloader:\n",
    "            image_batch_or = sample_batch[0].clone()\n",
    "            image_batch = torch.randn(image_batch_or.shape)\n",
    "            out = net(image_batch)\n",
    "            conc_out = torch.cat([conc_out, out.cpu()])\n",
    "            conc_label = torch.cat([conc_label, image_batch_or.cpu()])\n",
    "        val_loss = loss_fn(conc_out, conc_label)\n",
    "    max_val = val_loss.item()\n",
    "    return [min_val, max_val]\n",
    "\n",
    "def compute_limits_loss_crop(net, dataloader, loss_fn):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        conc_out = torch.Tensor().float()\n",
    "        conc_label = torch.Tensor().float()\n",
    "        for sample_batch in dataloader:\n",
    "            image_batch = sample_batch[0]\n",
    "            out = net(image_batch)\n",
    "            conc_out = torch.cat([conc_out, out.cpu()])\n",
    "            conc_label = torch.cat([conc_label, image_batch.cpu()])\n",
    "        val_loss = loss_fn(conc_out, conc_label)\n",
    "    min_val = val_loss.item()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        conc_out = torch.Tensor().float()\n",
    "        conc_label = torch.Tensor().float()\n",
    "        for sample_batch in dataloader:\n",
    "            image_batch_or = sample_batch[0].clone()\n",
    "            image_batch = torch.zeros(image_batch_or.shape)\n",
    "            out = net(image_batch)\n",
    "            conc_out = torch.cat([conc_out, out.cpu()])\n",
    "            conc_label = torch.cat([conc_label, image_batch_or.cpu()])\n",
    "        val_loss = loss_fn(conc_out, conc_label)\n",
    "    max_val = val_loss.item()\n",
    "    return [min_val, max_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_noise_crop(net, dataloader, loss_fn):\n",
    "    net.eval()\n",
    "    device = torch.device('cpu')\n",
    "    net.to(device)\n",
    "    \n",
    "    #Noise\n",
    "    limits_loss_noise = compute_limits_loss_noise(net, dataloader, loss_fn)\n",
    "    print('\\n@@@@@@@@@@@@@@@@@@@@@@@   Noise testing:\\n')\n",
    "    noise_try = [0, 0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.3, 0.4,0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.5, 2]\n",
    "    loss_log_noise = []\n",
    "    for noise_level in noise_try:\n",
    "        transform_noise = transforms.Lambda(lambda x: x + noise_level*torch.randn(x.shape))\n",
    "        with torch.no_grad():\n",
    "            conc_out = torch.Tensor().float()\n",
    "            conc_label = torch.Tensor().float()\n",
    "            for sample_batch in dataloader:\n",
    "                image_batch_or = sample_batch[0].clone()\n",
    "                image_batch = transform_noise(sample_batch[0])\n",
    "                out = net(image_batch)\n",
    "                conc_out = torch.cat([conc_out, out.cpu()])\n",
    "                conc_label = torch.cat([conc_label, image_batch_or.cpu()])\n",
    "            val_loss = loss_fn(conc_out, conc_label)-limits_loss_noise[0]\n",
    "            val_loss *= 100/(limits_loss_noise[1]-limits_loss_noise[0])\n",
    "            loss_log_noise.append(val_loss.item())\n",
    "            print('Test loss with a %.1f %% noise level: %.4f' %(noise_level*100, val_loss))\n",
    "            #fig, axs = plt.subplots(5,3 , figsize=(3,5))\n",
    "            #for index in range(5):\n",
    "            #    img = image_batch[index].view(1,-1,28,28)\n",
    "            #    rec_img  = net(img)\n",
    "            #    axs[index, 0].imshow(image_batch_or[index].cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "            #    axs[index,0].axis('off')\n",
    "            #    axs[index, 1].imshow(image_batch[index].cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "            #    axs[index,1].axis('off')\n",
    "            #    axs[index, 2].imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "            #    axs[index,2].axis('off')\n",
    "            #plt.tight_layout()\n",
    "            #plt.show()\n",
    "            plt.plot\n",
    "    plt.plot(noise_try, loss_log_noise)\n",
    "    plt.show()\n",
    "        \n",
    "    #Crop\n",
    "    limits_loss_crop = compute_limits_loss_crop(net, dataloader, loss_fn)\n",
    "    print('\\n@@@@@@@@@@@@@@@@@@@@@@@   Crop testing:\\n')\n",
    "    scale_try = [0, 0.01, 0.025, 0.05, 0.075, 0.1, 0.125, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "    loss_log_crop = []\n",
    "    for scale in scale_try:\n",
    "        transform_crop = transforms.RandomErasing(p=1, scale=(scale,scale), ratio=(0.5,2))\n",
    "        with torch.no_grad():\n",
    "            conc_out = torch.Tensor().float()\n",
    "            conc_label = torch.Tensor().float()\n",
    "            for sample_batch in dataloader:\n",
    "                image_batch_or = sample_batch[0].clone()\n",
    "                image_batch = sample_batch[0]\n",
    "                if scale!=0:\n",
    "                    for i in range(len(image_batch)):\n",
    "                        image_batch[i] = transform_crop(image_batch[i])\n",
    "                out = net(image_batch)\n",
    "                conc_out = torch.cat([conc_out, out.cpu()])\n",
    "                conc_label = torch.cat([conc_label, image_batch_or.cpu()])\n",
    "            val_loss = loss_fn(conc_out, conc_label)-limits_loss_crop[0]\n",
    "            val_loss *= 100/(limits_loss_crop[1]-limits_loss_crop[0])\n",
    "            loss_log_crop.append(val_loss.item())\n",
    "            print('Test loss with a %.1f %% crop: %.4f' %(scale*100, val_loss))\n",
    "            #fig, axs = plt.subplots(5, 3, figsize=(3,5))\n",
    "            #for index in range(5):\n",
    "            #    img = image_batch[index].view(1,-1,28,28)\n",
    "            #    rec_img  = net(img)\n",
    "            #    axs[index, 0].imshow(image_batch_or[index].cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "            #    axs[index,0].axis('off')\n",
    "            #    axs[index, 1].imshow(image_batch[index].cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "            #    axs[index,1].axis('off')\n",
    "            #    axs[index, 2].imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "            #    axs[index,2].axis('off')\n",
    "            #plt.tight_layout()\n",
    "            #plt.show()\n",
    "    plt.plot(scale_try, loss_log_crop)\n",
    "    plt.show()\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
